{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff034b9-55b2-4d3b-b4a7-3a9c2f7225aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set the corresponding values in the cell below. Afterwards, just run the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b1a2f2-a091-4efd-9477-0d158e5b4b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Class label csv path\n",
    "labels_csv_path = '../datasets/home_labels.csv'\n",
    "# Video example\n",
    "video_path = '../datasets/train/crying/_ceBK5pQTrs_000033_000043.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e621c",
   "metadata": {},
   "source": [
    "### Read class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b29c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "70 Unique classes: ['No people is in the room', 'Studying', 'Typing', 'Using computer', 'Making phone calls', 'Play with phone/tablet', 'Playing with pets', 'Feeding pets', 'Reading', 'Writing', 'Setting table', 'Eating food', 'Dining', 'Cooking', 'Sleeping', 'Brushing teeth', 'Showering', 'Playing music', 'TV', 'Cleaning', 'Doing laundry', 'Mopping floor', 'Vacumning', 'Ironing', 'Organizing space', 'Sewing', 'Knitting', 'Decorating', 'Party', 'Chatting', 'Talking', 'Singing', 'Laughing', 'Speaking', 'Dancing', 'Drinking', 'Stretching', 'Meditating', 'Drawing', 'Painting', 'Playing board games', 'Playing video games', 'Taking photos', 'Potluck', 'Working', 'Exercising', 'Walking', 'Running', 'Celebratin', 'Physical altercations', 'Verbal confrontations', 'Using drug', 'Theft or vandalism', 'Fighting', 'Domestic violence', 'Break in', 'Glass breaking', 'Fire accident', 'Fire alarm', 'Unattended cooking', 'Open flame', 'Smoking', 'Gunshot', 'Making noise', 'Falling down', 'Tripping', 'Crying', 'Suffocating', 'Furniture Collapse', 'Flooding'].\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "label2id = {}\n",
    "id2label = {}\n",
    "with open(labels_csv_path, mode='r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    for i, row in enumerate(csv_reader):\n",
    "        class_name = row[0]\n",
    "        label2id[class_name] = i\n",
    "        id2label[i] = class_name\n",
    "\n",
    "class_labels = list(label2id.keys())\n",
    "\n",
    "print(f\"{len(class_labels)} Unique classes: {class_labels}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a87b3-c1e8-4654-8762-ffd033544482",
   "metadata": {},
   "source": [
    "### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8d8a03-2bc5-4468-b682-eeb9f0fbcb8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felicialuo/opt/anaconda3/envs/vclap/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/felicialuo/opt/anaconda3/envs/vclap/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.config import get_config\n",
    "from utils.logger import create_logger\n",
    "import numpy as np\n",
    "from utils.config import get_config\n",
    "from trainers import vificlip\n",
    "from datasets.pipeline import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070467e",
   "metadata": {},
   "source": [
    "### Setting up configuration, no need to change anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78418d43-ddd6-460d-8c30-7884a6383c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from configs/zero_shot/train/k400/16_16_vifi_clip.yaml\n",
      "\u001b[32m[2024-04-16 18:30:14 ViT-B/16]\u001b[0m\u001b[33m(1413150786.py 23)\u001b[0m: INFO working dir: outputs\n"
     ]
    }
   ],
   "source": [
    "config = 'configs/zero_shot/train/k400/16_16_vifi_clip.yaml'\n",
    "output_folder_name = \"outputs\"\n",
    "pretrained_model_path = \"ckpts/vifi_clip_10_epochs_k400_full_finetuned.pth\"\n",
    "\n",
    "# Step 1:\n",
    "# Configuration class \n",
    "class parse_option():\n",
    "    def __init__(self):\n",
    "        self.config = config\n",
    "        self.output =  output_folder_name   # Name of output folder to store logs and save weights\n",
    "        self.resume = pretrained_model_path\n",
    "        # No need to change below args.\n",
    "        self.only_test = True\n",
    "        self.opts = None\n",
    "        self.batch_size = None\n",
    "        self.pretrained = None\n",
    "        self.accumulation_steps = None\n",
    "        self.local_rank = 0\n",
    "args = parse_option()\n",
    "config = get_config(args)\n",
    "# logger\n",
    "logger = create_logger(output_dir=args.output, name=f\"{config.MODEL.ARCH}\")\n",
    "logger.info(f\"working dir: {config.OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda20686-733a-463b-8dbb-1b4bd1cb1eda",
   "metadata": {},
   "source": [
    "### Loading ViFi-CLIP and its pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7d8db5-35c7-4c42-82bb-2c58257f61a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-04-16 18:30:14 ViT-B/16]\u001b[0m\u001b[33m(vificlip.py 203)\u001b[0m: INFO Loading CLIP (backbone: ViT-B/16)\n",
      "\u001b[32m[2024-04-16 18:30:17 ViT-B/16]\u001b[0m\u001b[33m(vificlip.py 206)\u001b[0m: INFO Building ViFi-CLIP CLIP\n",
      "\u001b[32m[2024-04-16 18:30:17 ViT-B/16]\u001b[0m\u001b[33m(vificlip.py 223)\u001b[0m: INFO Turning on gradients for COMPLETE ViFi-CLIP model\n",
      "\u001b[32m[2024-04-16 18:30:17 ViT-B/16]\u001b[0m\u001b[33m(vificlip.py 246)\u001b[0m: INFO Parameters to be updated: {'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'text_encoder.transformer.resblocks.3.ln_2.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.ln_final.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'logit_scale', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'text_encoder.ln_final.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'image_encoder.ln_post.bias', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'image_encoder.positional_embedding', 'image_encoder.conv1.weight', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'image_encoder.transformer.resblocks.2.ln_2.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'image_encoder.proj', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.positional_embedding', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'image_encoder.ln_pre.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.3.ln_2.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.text_projection', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'image_encoder.ln_pre.weight', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'image_encoder.class_embedding', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.ln_post.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias'}\n",
      "\u001b[32m[2024-04-16 18:30:17 ViT-B/16]\u001b[0m\u001b[33m(vificlip.py 247)\u001b[0m: INFO Total learnable items: 301\n"
     ]
    }
   ],
   "source": [
    "# Step 2:\n",
    "# Create the ViFi-CLIP models and load pretrained weights\n",
    "model = vificlip.returnCLIP(config,\n",
    "                            logger=logger,\n",
    "                            class_names=class_labels,)\n",
    "model = model.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e321c8-8214-4010-8ee6-5d57def27c61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-04-16 18:30:17 ViT-B/16]\u001b[0m\u001b[33m(2665388570.py 1)\u001b[0m: INFO ==============> Resuming form ckpts/vifi_clip_10_epochs_k400_full_finetuned.pth....................\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"==============> Resuming form {config.MODEL.RESUME}....................\")\n",
    "checkpoint = torch.load(config.MODEL.RESUME, map_location='cpu')\n",
    "load_state_dict = checkpoint['model']\n",
    "# now remove the unwanted keys:\n",
    "if \"module.prompt_learner.token_prefix\" in load_state_dict:\n",
    "    del load_state_dict[\"module.prompt_learner.token_prefix\"]\n",
    "\n",
    "if \"module.prompt_learner.token_suffix\" in load_state_dict:\n",
    "    del load_state_dict[\"module.prompt_learner.token_suffix\"]\n",
    "\n",
    "if \"module.prompt_learner.complete_text_embeddings\" in load_state_dict:\n",
    "    del load_state_dict[\"module.prompt_learner.complete_text_embeddings\"]\n",
    "# create new OrderedDict that does not contain `module.`\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in load_state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13867cfa-876a-498e-b324-218e8a8bcdf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-04-16 18:30:18 ViT-B/16]\u001b[0m\u001b[33m(3852643250.py 3)\u001b[0m: INFO resume model: _IncompatibleKeys(missing_keys=['prompt_learner.complete_text_embeddings'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "# load params\n",
    "msg = model.load_state_dict(new_state_dict, strict=False)\n",
    "logger.info(f\"resume model: {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b0f3f-541c-4009-945f-977cc4032f0d",
   "metadata": {},
   "source": [
    "### Preprocessing input video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa85460-c71e-47be-8dbd-d3e181a633d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: \n",
    "# Preprocessing for video\n",
    "img_norm_cfg = dict(\n",
    "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
    "scale_resize = int(256 / 224 * config.DATA.INPUT_SIZE)\n",
    "val_pipeline = [\n",
    "    dict(type='DecordInit'),\n",
    "    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=config.DATA.NUM_FRAMES, test_mode=True),\n",
    "    dict(type='DecordDecode'),\n",
    "    dict(type='Resize', scale=(-1, scale_resize)),\n",
    "    dict(type='CenterCrop', crop_size=config.DATA.INPUT_SIZE),\n",
    "    dict(type='Normalize', **img_norm_cfg),\n",
    "    dict(type='FormatShape', input_format='NCHW'),\n",
    "    dict(type='Collect', keys=['imgs'], meta_keys=[]),\n",
    "    dict(type='ToTensor', keys=['imgs'])\n",
    "]\n",
    "if config.TEST.NUM_CROP == 3:\n",
    "    val_pipeline[3] = dict(type='Resize', scale=(-1, config.DATA.INPUT_SIZE))\n",
    "    val_pipeline[4] = dict(type='ThreeCrop', crop_size=config.DATA.INPUT_SIZE)\n",
    "if config.TEST.NUM_CLIP > 1:\n",
    "    val_pipeline[1] = dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=config.DATA.NUM_FRAMES, multiview=config.TEST.NUM_CLIP)\n",
    "pipeline = Compose(val_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb069f4-6adb-409e-936b-3b2992947d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_file = {'filename': video_path, 'tar': False, 'modality': 'RGB', 'start_index': 0, 'gt_label': video_path.split('/')[-2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52bb28-c681-479a-89f3-fc86362b6d3f",
   "metadata": {},
   "source": [
    "### ViFi-CLIP inference with given video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ef8028a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[5499]: Class AVFFrameReceiver is implemented in both /Users/felicialuo/opt/anaconda3/envs/vclap/lib/python3.11/site-packages/av/.dylibs/libavdevice.60.1.100.dylib (0x13fa320f8) and /Users/felicialuo/opt/anaconda3/envs/vclap/lib/python3.11/site-packages/decord/.dylibs/libavdevice.58.5.100.dylib (0x155a2d010). One of the two will be used. Which one is undefined.\n",
      "objc[5499]: Class AVFAudioReceiver is implemented in both /Users/felicialuo/opt/anaconda3/envs/vclap/lib/python3.11/site-packages/av/.dylibs/libavdevice.60.1.100.dylib (0x13fa32148) and /Users/felicialuo/opt/anaconda3/envs/vclap/lib/python3.11/site-packages/decord/.dylibs/libavdevice.58.5.100.dylib (0x155a2d060). One of the two will be used. Which one is undefined.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "video = pipeline(dict_file)\n",
    "video_tensor = video['imgs'].unsqueeze(0).float()\n",
    "# Inference through ViFi-CLIP\n",
    "with torch.no_grad():\n",
    "    # with torch.cuda.amp.autocast():\n",
    "    similarities = model(video_tensor)\n",
    "\n",
    "similarity = F.softmax(similarities, dim=1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "pred_index = indices[0]\n",
    "\n",
    "# Print the results\n",
    "print(\"Ground Truth: {}\".format(video_path.split('/')[-2]))\n",
    "print(\"Top predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{class_labels[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
